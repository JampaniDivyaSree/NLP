{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JampaniDivyaSree/NLP/blob/main/NLP_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS3hWIuJEe9-"
      },
      "source": [
        "## Introduction to Natural Language Processing\n",
        "\n",
        "In this workbook, at a high-level we will learn about text tokenization; text normalization such as lowercasing, stemming; part-of-speech tagging; Named entity recognition;\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkXwl0p1G6Gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb40596d-37c1-4e4f-a6e2-31793e60a075"
      },
      "source": [
        "####PLEASE EXECUTE THESE COMMANDS BEFORE PROCEEDING####\n",
        "import nltk  #Natural Language Toolkit\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_pNvP0lDAZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd148046-a2c2-4923-fcac-5906c50e318c"
      },
      "source": [
        "#Tokenization -- Text into word tokens; Paragraphs into sentences;\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning the important basics of NLP.\"\n",
        "sent_tokenize(text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to Intro to Machine Learning Applications.',\n",
              " 'We are now learning the important basics of NLP.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqkXKzNOG_CP"
      },
      "source": [
        "import nltk.data\n",
        "\n",
        "german_tokenizer = nltk.data.load('tokenizers/punkt/PY3/german.pickle')\n",
        "\n",
        "text = 'Wie geht es Ihnen? Mir geht es gut.'\n",
        "german_tokenizer.tokenize(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMAa2dymH7f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "979a820a-6145-4289-e7d7-17d93255628d"
      },
      "source": [
        "#Tokenization -- Text into word tokens; Paragraphs into words;\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
        "word_tokenize(text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'everyone',\n",
              " '.',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'Intro',\n",
              " 'to',\n",
              " 'Machine',\n",
              " 'Learning',\n",
              " 'Applications',\n",
              " '.',\n",
              " 'We',\n",
              " 'are',\n",
              " 'now',\n",
              " 'learning',\n",
              " 'important',\n",
              " 'basics',\n",
              " 'of',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdYqZcYCINaK"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yFvaxNeTc-r"
      },
      "source": [
        "###n-grams vs tokens\n",
        "\n",
        "##### n-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.\n",
        "\n",
        "##### Tokens do not have any conditions on contiguity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_3jEcxcVbA-"
      },
      "source": [
        "#Using pure python\n",
        "\n",
        "import re\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    # Convert to lowercases\n",
        "    text = text.lower()\n",
        "\n",
        "    # Replace all none alphanumeric characters with spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "\n",
        "    # Break sentence in the token, remove empty tokens\n",
        "    tokens = [token for token in text.split(\" \") if token != \"\"]\n",
        "\n",
        "    # Use the zip function to help us generate n-grams\n",
        "    # Concatentate the tokens into ngrams and return\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    return [\" \".join(ngram) for ngram in ngrams]\n",
        "\n",
        "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
        "print(text)\n",
        "generate_ngrams(text, n=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz-Mq1T6YQSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f438f12-a61e-4c66-f4f8-c81c185e9395"
      },
      "source": [
        "#Using NLTK import ngrams\n",
        "\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "tokens = [token for token in text.split(\" \") if token != \"\"]\n",
        "output = list(ngrams(tokens, 3))\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hello', 'everyone', 'welcome'), ('everyone', 'welcome', 'to'), ('welcome', 'to', 'intro'), ('to', 'intro', 'to'), ('intro', 'to', 'machine'), ('to', 'machine', 'learning'), ('machine', 'learning', 'applications'), ('learning', 'applications', 'we'), ('applications', 'we', 'are'), ('we', 'are', 'now'), ('are', 'now', 'learning'), ('now', 'learning', 'important'), ('learning', 'important', 'basics'), ('important', 'basics', 'of'), ('basics', 'of', 'nlp')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BG909xTFbeZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195a646c-2f6d-46bf-d2e7-3af98705a804"
      },
      "source": [
        "#Text Normalization\n",
        "\n",
        "#Lowercasing\n",
        "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
        "lowert = text.lower()\n",
        "uppert = text.upper()\n",
        "\n",
        "print(lowert)\n",
        "print(uppert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello everyone. welcome to intro to machine learning applications. we are now learning important basics of nlp.\n",
            "HELLO EVERYONE. WELCOME TO INTRO TO MACHINE LEARNING APPLICATIONS. WE ARE NOW LEARNING IMPORTANT BASICS OF NLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JxdoZyaY-iP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973ddf45-ef95-4fec-87f9-ad18880fca0b"
      },
      "source": [
        "#Text Normalization\n",
        "#stemming\n",
        "#Porter stemmer is a famous stemming approach\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# choose some words to be stemmed\n",
        "words = [\"hike\", \"hikes\", \"hiked\", \"hiking\", \"hikers\", \"hiker\"]\n",
        "\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hike  :  hike\n",
            "hikes  :  hike\n",
            "hiked  :  hike\n",
            "hiking  :  hike\n",
            "hikers  :  hiker\n",
            "hiker  :  hiker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6EM6ADdZYbL"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "ps = PorterStemmer()\n",
        "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
        "print(text)\n",
        "\n",
        "\n",
        "#Tokenize and stem the words\n",
        "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "tokens = [token for token in text.split(\" \") if token != \"\"]\n",
        "\n",
        "i=0\n",
        "while i<len(tokens):\n",
        "  tokens[i]=ps.stem(tokens[i])\n",
        "  i=i+1\n",
        "\n",
        "#merge all the tokens to form a long text sequence\n",
        "text2 = ' '.join(tokens)\n",
        "\n",
        "print(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQg-2u17aWQh"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
        "print(text)\n",
        "\n",
        "\n",
        "#Tokenize and stem the words\n",
        "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "tokens = [token for token in text.split(\" \") if token != \"\"]\n",
        "\n",
        "i=0\n",
        "while i<len(tokens):\n",
        "  tokens[i]=ss.stem(tokens[i])\n",
        "  i=i+1\n",
        "\n",
        "#merge all the tokens to form a long text sequence\n",
        "text2 = ' '.join(tokens)\n",
        "\n",
        "print(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQkySHTBldBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50418429-c4fb-47f5-cbce-f25a288996fa"
      },
      "source": [
        " #Stopwords removal\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)\n",
        "\n",
        "text2 = ' '.join(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'Welcome', 'to', 'Intro', 'to', 'Machine', 'Learning', 'Applications', '.', 'We', 'are', 'now', 'learning', 'important', 'basics', 'of', 'NLP', '.']\n",
            "['Hello', 'everyone', '.', 'Welcome', 'Intro', 'Machine', 'Learning', 'Applications', '.', 'We', 'learning', 'important', 'basics', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejWwVdZebHlA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b29298-1c27-487d-b4d4-9c68cea80ded",
        "collapsed": true
      },
      "source": [
        "#Part-of-Speech tagging -\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = 'Rudolph Smith bought 1000 shares of tesla inc in May 2022'\n",
        "\n",
        "def preprocess(sent):\n",
        "    sent = nltk.word_tokenize(sent)\n",
        "    sent = nltk.pos_tag(sent)\n",
        "    return sent\n",
        "\n",
        "sent = preprocess(text)\n",
        "print(sent)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Rudolph', 'NNP'), ('Smith', 'NNP'), ('bought', 'VBD'), ('1000', 'CD'), ('shares', 'NNS'), ('of', 'IN'), ('tesla', 'NN'), ('inc', 'NN'), ('in', 'IN'), ('May', 'NNP'), ('2022', 'CD')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nQvO9BTFdGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36c7e9c-d97a-429f-8433-5b9ed3c1e3b7",
        "collapsed": true
      },
      "source": [
        "#Named entity recognition\n",
        "#spaCy is an NLP Framework -- easy to use and having ability to use neural networks\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "text = 'Mr.Rudolph Smith bought 1000 shares of Tesla inc in May 2024'\n",
        "\n",
        "doc = nlp(text)\n",
        "print(doc.ents)\n",
        "print([(X.text, X.label_) for X in doc.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Rudolph Smith, 1000, Tesla inc, May 2024)\n",
            "[('Rudolph Smith', 'PERSON'), ('1000', 'CARDINAL'), ('Tesla inc', 'ORG'), ('May 2024', 'DATE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg-BTKVxFgSP"
      },
      "source": [
        "#Sentiment analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FcpVeeZFiA4"
      },
      "source": [
        "#Topic modeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0-mQ_AMFlmn"
      },
      "source": [
        "#Word embeddings\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ5DZ_ZkI74h"
      },
      "source": [
        "#Class exercise\n",
        "\n",
        "#### 1. Read a file from its URL\n",
        "#### 2. Extract the text and tokenize it meaningfully into words.\n",
        "#### 3. Print the entire text combined after tokenization.\n",
        "#### 4. Perform stemming using both porter and snowball stemmers. Which one works the best? Why?\n",
        "#### 5. Remove stopwords\n",
        "#### 6. Identify the top-10 unigrams based on their frequency.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHvXv_uILCvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279888b4-8d2d-4928-df20-09494aab96bc"
      },
      "source": [
        "\n",
        "#Load the file first\n",
        "!wget https://www.dropbox.com/s/o8lxi6yrezmt5em/reviews.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-28 13:25:56--  https://www.dropbox.com/s/o8lxi6yrezmt5em/reviews.txt\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6022:18::a27d:4212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/o8lxi6yrezmt5em/reviews.txt [following]\n",
            "--2021-11-28 13:25:57--  https://www.dropbox.com/s/raw/o8lxi6yrezmt5em/reviews.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucafc5669651649180f2312c34bd.dl.dropboxusercontent.com/cd/0/inline/Ba0oeZqKZ-brxGhb21ApjGMDrYkS3dPabH3kI2nCZF0WGsw_2Bo0pt46GBn8laTbq6T_3cB6R_K-BQnCJHx8GrvySVoDqJysqyThcNxnB9VtweIJ_ei1Zl1ZxjuB3IuNy1DHll3E4ngfE_kIpsCVNrlQ/file# [following]\n",
            "--2021-11-28 13:25:57--  https://ucafc5669651649180f2312c34bd.dl.dropboxusercontent.com/cd/0/inline/Ba0oeZqKZ-brxGhb21ApjGMDrYkS3dPabH3kI2nCZF0WGsw_2Bo0pt46GBn8laTbq6T_3cB6R_K-BQnCJHx8GrvySVoDqJysqyThcNxnB9VtweIJ_ei1Zl1ZxjuB3IuNy1DHll3E4ngfE_kIpsCVNrlQ/file\n",
            "Resolving ucafc5669651649180f2312c34bd.dl.dropboxusercontent.com (ucafc5669651649180f2312c34bd.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to ucafc5669651649180f2312c34bd.dl.dropboxusercontent.com (ucafc5669651649180f2312c34bd.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3851 (3.8K) [text/plain]\n",
            "Saving to: ‘reviews.txt’\n",
            "\n",
            "reviews.txt         100%[===================>]   3.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-28 13:25:57 (424 MB/s) - ‘reviews.txt’ saved [3851/3851]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}